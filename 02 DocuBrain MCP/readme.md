# ðŸ“„ DocuBrain â€” MCP Document Summarizer  
### (Built with MCP + Sampling + OpenAI Agent SDK + Gemini)

DocuBrain is a complete end-to-end project showing how to build an **MCP Server + MCP Client** that uses **Sampling** to run an LLM-powered Summarizer Agent via the **OpenAI Agent SDK**.  
It supports **local documents, remote URLs, prompts**, and real-time **LLM summarization using Gemini**.

---

## ðŸš€ Features

- ðŸ”§ **MCP Tools** â†’ List documents, read documents, summarize text  
- ðŸ“¡ **MCP Resources** â†’ Load documents dynamically (local + remote)  
- ðŸ“˜ **MCP Prompts** â†’ Reusable summarization templates  
- ðŸ§  **Sampling** â†’ Server defers summarization to the client  
- ðŸ¤– **Agent SDK Integration** â†’ Summarizer Agent using `gemini-2.5-flash`  
- âš¡ **Async I/O** â†’ Uses `aiofiles` + `httpx` for fast file/network access  


## Project Structure

ðŸ“‚ documind_mcp/
 â”£ ðŸ“œ server.py           # Main MCP server with tools and resources
 â”£ ðŸ“œ client.py           # MCP client to interact with tools
 â”£ ðŸ“‚ documents/          # Local .txt / .md / .pdf files
 â”£ ðŸ“œ prompts.yaml        # Summarization prompt template
 â”— ðŸ“œ .env                # Any needed configs

## ðŸ›  MCP Server (server.py)

### Server Capabilities
- **`list_documents`** â†’ Returns available docs + size  
- **`read_document`** â†’ Reads local files or remote URLs  
- **`summarize_document`** â†’ Prompt template  
- **`summarize_content`** â†’ Calls LLM via Sampling  

### ðŸ”„ How the Server Works (Flow)
1. Client calls **summarize_content** tool  
2. Server creates a **SamplingMessage**  
3. MCP client receives it and triggers **real_summarize**  
4. Summarizer Agent runs via OpenAI Agent SDK  
5. Agent produces a summary  
6. Summary sent back to MCP server  
7. Server returns it as tool output  

---

## ðŸ§  MCP Client (client.py)

### Client Responsibilities
- Connects to the MCP server  
- Registers a **sampling callback** (`real_summarize`)  
- Creates a **Summarizer Agent** using Gemini  
- Runs the agent using **Runner.run()**  
- Sends back LLM output to MCP  

---
